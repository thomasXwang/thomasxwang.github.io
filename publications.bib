
@article{wang_handling_2023,
	title = {Handling unstructured data for operator learning using implicit neural representations},
	url = {https://openreview.net/forum?id=e2gSQqH3V10&referrer=%5Bthe%20profile%20of%20Thomas%20X%20Wang%5D(%2Fprofile%3Fid%3D~Thomas_X_Wang1)},
	abstract = {Operator learning methods are too often constrained by a fixed sampling of both the input and output functions. We propose a novel method to allow current operator learning methods to learn on any sampling. We show that our method can perform inference on unseen samplings, and that it allows returning outputs as continuous functions.},
	language = {en},
	urldate = {2023-10-01},
	author = {Wang, Thomas X. and Gallinari, Patrick},
	month = mar,
	year = {2023},
	file = {Full Text PDF:/Users/thomas/Zotero/storage/6G8PM5LT/Wang et Gallinari - 2023 - Handling unstructured data for operator learning u.pdf:application/pdf},
}

@misc{serrano_operator_2023,
	title = {Operator {Learning} with {Neural} {Fields}: {Tackling} {PDEs} on {General} {Geometries}},
	shorttitle = {Operator {Learning} with {Neural} {Fields}},
	url = {http://arxiv.org/abs/2306.07266},
	doi = {10.48550/arXiv.2306.07266},
	abstract = {Machine learning approaches for solving partial differential equations require learning mappings between function spaces. While convolutional or graph neural networks are constrained to discretized functions, neural operators present a promising milestone toward mapping functions directly. Despite impressive results they still face challenges with respect to the domain geometry and typically rely on some form of discretization. In order to alleviate such limitations, we present CORAL, a new method that leverages coordinate-based networks for solving PDEs on general geometries. CORAL is designed to remove constraints on the input mesh, making it applicable to any spatial sampling and geometry. Its ability extends to diverse problem domains, including PDE solving, spatio-temporal forecasting, and inverse problems like geometric design. CORAL demonstrates robust performance across multiple resolutions and performs well in both convex and non-convex domains, surpassing or performing on par with state-of-the-art models.},
	urldate = {2023-10-01},
	publisher = {arXiv},
	author = {Serrano, Louis and Boudec, Lise Le and Koupaï, Armand Kassaï and Wang, Thomas X. and Yin, Yuan and Vittaut, Jean-Noël and Gallinari, Patrick},
	month = jun,
	year = {2023},
	note = {arXiv:2306.07266 [cs]},
	file = {arXiv Fulltext PDF:/Users/thomas/Zotero/storage/MP42GBL8/Serrano et al. - 2023 - Operator Learning with Neural Fields Tackling PDE.pdf:application/pdf;arXiv.org Snapshot:/Users/thomas/Zotero/storage/Z24Q3NDI/2306.html:text/html},
}

@misc{serrano_aroma_2024,
	title = {{AROMA}: {Preserving} {Spatial} {Structure} for {Latent} {PDE} {Modeling} with {Local} {Neural} {Fields}},
	shorttitle = {{AROMA}},
	url = {http://arxiv.org/abs/2406.02176},
	doi = {10.48550/arXiv.2406.02176},
	abstract = {We present AROMA (Attentive Reduced Order Model with Attention), a framework designed to enhance the modeling of partial differential equations (PDEs) using local neural fields. Our flexible encoder-decoder architecture can obtain smooth latent representations of spatial physical fields from a variety of data types, including irregular-grid inputs and point clouds. This versatility eliminates the need for patching and allows efficient processing of diverse geometries. The sequential nature of our latent representation can be interpreted spatially and permits the use of a conditional transformer for modeling the temporal dynamics of PDEs. By employing a diffusion-based formulation, we achieve greater stability and enable longer rollouts compared to conventional MSE training. AROMA's superior performance in simulating 1D and 2D equations underscores the efficacy of our approach in capturing complex dynamical behaviors.},
	urldate = {2024-07-31},
	publisher = {arXiv},
	author = {Serrano, Louis and Wang, Thomas X. and Naour, Etienne Le and Vittaut, Jean-Noël and Gallinari, Patrick},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02176 [cs]},
	file = {arXiv Fulltext PDF:/Users/thomas/Zotero/storage/CRX3JBXX/Serrano et al. - 2024 - AROMA Preserving Spatial Structure for Latent PDE.pdf:application/pdf;arXiv.org Snapshot:/Users/thomas/Zotero/storage/9FE56SRU/2406.html:text/html},
}

@misc{saratchandran_weight_2024,
	title = {Weight {Conditioning} for {Smooth} {Optimization} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/2409.03424},
	doi = {10.48550/arXiv.2409.03424},
	abstract = {In this article, we introduce a novel normalization technique for neural network weight matrices, which we term weight conditioning. This approach aims to narrow the gap between the smallest and largest singular values of the weight matrices, resulting in better-conditioned matrices. The inspiration for this technique partially derives from numerical linear algebra, where well-conditioned matrices are known to facilitate stronger convergence results for iterative solvers. We provide a theoretical foundation demonstrating that our normalization technique smoothens the loss landscape, thereby enhancing convergence of stochastic gradient descent algorithms. Empirically, we validate our normalization across various neural network architectures, including Convolutional Neural Networks (CNNs), Vision Transformers (ViT), Neural Radiance Fields (NeRF), and 3D shape modeling. Our findings indicate that our normalization method is not only competitive but also outperforms existing weight normalization techniques from the literature.},
	urldate = {2024-09-28},
	publisher = {arXiv},
	author = {Saratchandran, Hemanth and Wang, Thomas X. and Lucey, Simon},
	month = sep,
	year = {2024},
	note = {arXiv:2409.03424 [cs]},
	file = {arXiv Fulltext PDF:/Users/thomas/Zotero/storage/FDCEQ2K9/Saratchandran et al. - 2024 - Weight Conditioning for Smooth Optimization of Neu.pdf:application/pdf;arXiv.org Snapshot:/Users/thomas/Zotero/storage/GT973UCT/2409.html:text/html},
}

@inproceedings{wang_autobasisencoder_2024,
	title = {{AutoBasisEncoder}: {Pre}-trained {Neural} {Field} {Basis} via {Autoencoding} for {Operator} {Learning}},
	shorttitle = {{AutoBasisEncoder}},
	url = {https://openreview.net/forum?id=8bVPkfswQG},
	abstract = {We introduce AutoBasisEncoder, a novel framework designed for operator learn- ing – the task of learning to map from one function to another. This approach au- tonomously discovers a basis of functions optimized for the target function space and utilizes this pre-trained basis for efficient operator learning. By introducing an intermediary auto-encoding task to the popular DeepONet framework, AutoBa- sisEncoder disentangles the learning of the basis functions and of the coefficients, simplifying the operator learning process. Initially, the framework learns basis functions through auto-encoding, followed by leveraging this basis to predict the coefficients of the target function. Preliminary experiments indicate that Auto- BasisEncoder’s basis functions exhibit superior suitability for operator learning and function reconstruction compared to DeepONet. These findings underscore the potential of AutoBasisEncoder to enhance the landscape of operator learning frameworks},
	language = {en},
	urldate = {2024-09-28},
	author = {Wang, Thomas X. and Baskiotis, Nicolas and Gallinari, Patrick},
	month = mar,
	year = {2024},
	file = {Full Text PDF:/Users/thomas/Zotero/storage/PAXCLNCI/Wang et al. - 2024 - AutoBasisEncoder Pre-trained Neural Field Basis v.pdf:application/pdf},
}
